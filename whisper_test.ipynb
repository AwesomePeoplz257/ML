{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from model_loader import *\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "# dangerous to run but needed\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing base model\n",
    "# pipe = pipeline(model=\"openai/whisper-tiny\")  # change to\n",
    "#\n",
    "# def transcribe(audio):\n",
    "#     text = pipe(audio)[\"text\"]\n",
    "#     return text\n",
    "#\n",
    "# iface = gr.Interface(\n",
    "#     fn=transcribe,\n",
    "#     inputs=gr.Audio(source=\"microphone\"),\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Whisper Small Hindi\",\n",
    "#     description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    "# )\n",
    "#\n",
    "# iface.launch()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "samplerate, audio = wavfile.read('000010001.WAV')\n",
    "print(samplerate)\n",
    "print(audio)\n",
    "pipe = pipeline(model=\"openai/whisper-tiny\")\n",
    "print('pipedone')\n",
    "text = pipe(audio)['text']\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# testing lora model\n",
    "\n",
    "lora_model = ws.from_pretrained(\"openai/whisper-small\")\n",
    "lora_model, params = lora_init(lora_model)\n",
    "lora_model, params = lora_init(lora_model, modules=params)\n",
    "\n",
    "pipe = pipeline(model = model)\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "competition code for fine tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from model_loader import *\n",
    "import torch\n",
    "from transformers import pipeline, WhisperFeatureExtractor, WhisperTokenizer\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import WhisperProcessor\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from word2number import w2n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:\\\\Users\\\\hojon\\\\OneDrive\\\\Desktop\\\\IRS Data\\\\TIL\\\\comps data\\\\Train.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\hojon\\\\OneDrive\\\\Desktop\\\\IRS Data\\\\TIL\\\\comps data\\\\Test_Novice_19May.csv\")\n",
    "\n",
    "\n",
    "def preprocess_data(df, local_path):\n",
    "    \"\"\"\n",
    "    :param df:\n",
    "    :param local_path: path to audio train files\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # converting annotations to lowercase\n",
    "    df['annotation'] = [annotation.lower() for annotation in df['annotation']]\n",
    "\n",
    "    # changing the numbers (letter form) to digits\n",
    "    temp = []\n",
    "    word2num = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9,\n",
    "                'ten': 10, 'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16,\n",
    "                'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20, 'thirty': 30, 'forty': 40, 'fifty': 50,\n",
    "                'sixty': 60, 'seventy': 70, 'eighty': 80, 'ninety': 90, 'zero': 0}\n",
    "    for annotation in df['annotation']:\n",
    "        sentence_added = False\n",
    "        for word in word2num.keys():\n",
    "            if word in annotation and sentence_added == False:\n",
    "                index = annotation.index(word)\n",
    "                new_annotation = annotation[:index] + str(word2num[word]) + annotation[index + len(word):]\n",
    "                temp.append(new_annotation)\n",
    "                sentence_added = True\n",
    "\n",
    "        if sentence_added == False:\n",
    "            temp.append(annotation)\n",
    "\n",
    "    df['annotation'] = temp\n",
    "\n",
    "    # making the first letter uppercase, add full stop to the back\n",
    "    temp = []\n",
    "    for annotation in df['annotation']:\n",
    "        new_annotation = annotation[0].upper() + annotation[1:] + \".\"\n",
    "        temp.append(new_annotation)\n",
    "    df['annotation'] = temp\n",
    "\n",
    "    # changing the filepaths\n",
    "    temp = []\n",
    "    for path in df['path']:\n",
    "        new_path = local_path + path[6:]\n",
    "\n",
    "        temp.append(new_path)\n",
    "    df['path'] = temp\n",
    "\n",
    "    audio_dataset = Dataset.from_dict({\"audio\": df['path'], \"annotation\": df['annotation']}).cast_column(\"audio\", Audio())\n",
    "\n",
    "    return audio_dataset\n",
    "\n",
    "\n",
    "audio_dataset = preprocess_data(train, \"C:\\\\Users\\\\hojon\\\\OneDrive\\\\Desktop\\\\IRS Data\\\\TIL\\\\comps data\\\\Train\\\\Train\\\\audio\\\\\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess_data(batch):\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")\n",
    "\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    input_features = []\n",
    "    for instance in audio:\n",
    "        input_features.append(feature_extractor(instance['array'], sampling_rate=16000).input_features[0])\n",
    "    batch[\"input_features\"] = input_features\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"annotation\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "trainset = audio_dataset[:int(3750 * 0.8)]\n",
    "testset = audio_dataset[int(3750 * 0.8):]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'annotation', 'input_features', 'labels'],\n",
      "    num_rows: 750\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "testset = preprocess_data(testset)\n",
    "testset = Dataset.from_dict(testset)\n",
    "trainset = preprocess_data(trainset)\n",
    "trainset = Dataset.from_dict(trainset)\n",
    "print(trainset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "testset = testset.remove_columns(['audio', 'annotation'])\n",
    "trainset = trainset.remove_columns(['audio', 'annotation'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "3000"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset[0]['input_features'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-finetunev1\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=30,\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hojon\\PycharmProjects\\noobsbelow\\ML\\stt\\huggingface-version\\./whisper-small-finetunev1 is already a clone of https://huggingface.co/casual/whisper-small-finetunev1. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hojon\\anaconda3\\envs\\AI_env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/4000 : < :, Epoch 0.01/22]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
